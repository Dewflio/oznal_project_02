# OZNAL - Project 02 - More classification and regression models

## FIIT STU - Bratislava

## April 2024

## Authors

-   Leonard Puškáč
-   Ema Richnáková

## Dataset

In this mini-project we will analyse data from <https://www.kaggle.com/datasets/julianoorlandi/spotify-top-songs-and-audio-features?resource=download>

The dataset contains information about various "top" songs and their stats on Spotify. There are 6513 entries each containing FILL IN features, such as the name of the song, its id, features that can be used to classify the songs such as the key or the mode (Major/Minor), as well as numeric features that can be used for regression analysis - energy, danceability, speechiness (the amount of spoken word), liveness, loudness, tempo and many others.

### Data Description

| Column name      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|------------------------------------|------------------------------------|
| id               | The Spotify ID for the track.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| artist_names     | The name of the artist.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| track_name       | The name of the track.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| source           | The name of the record label.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| key              | The key the track is in.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| mode             | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived.                                                                                                                                                                                                                                                                                                                                                                                              |
| time_signature   | An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7.                                                                                                                                                                                                                                                                                                                                  |
| danceability     | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.                                                                                                                                                                                                                                                                       |
| energy           | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.                                                                                                                          |
| speechiness      | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |
| acousticness     | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.                                                                                                                                                                                                                                                                                                                                                                                       |
| instrumentalness | Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.                                                                                                                 |
| liveness         | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.                                                                                                                                                                                                                                                                                            |
| valence          | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).                                                                                                                                                                                                                                                                  |
| loudness         | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.                                                                                                                                                                                     |
| tempo            | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.                                                                                                                                                                                                                                                                                                                         |
| duration_ms      | The duration of the track in milliseconds.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| weeks_on_chart   | Number of weeks the track was in the top 200 charts.                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| streams          | How many streams the track had during its period in the charts.                                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Importing Libraries

```{r message=FALSE}
library(magrittr)
library(tidyverse)
library(dplyr) # mutate
library(e1071) # SVM
library(pROC) # create ROC curve roc()
library(caret) # confusion matrix
```

## Loading the Dataset

```{r}
data <- read_csv("spotify_top_songs_audio_features.csv", col_names = TRUE, num_threads = 4)
head(data)
```

## Data exploration

### Correlation analysis

From contructed correlation heatmap below, we can see e.g. strong positive correlation between energy and loudness and valence. From this information, hypotheses can be created.

```{r}
heatmap(cor(data %>% select_if(is.numeric)),
        col = colorRampPalette(c("darkgreen", "white", "red"))(100),
        symm = TRUE)
```

#### Hypothesis 1

*Songs classified as more energetic (\>=0.64)(1) are more likely to be louder and happier compared to songs classified as less energetic (\<0.64)(class 0).*

Using energy mean as threshold for classification is beneficial for top Spotify songs dataset, where higher energy levels are prevalent. Classification classes will be more balanced.

```{r}
mean_energy <- mean(data$energy)
mean_energy
print(paste("Difference between mean and median:", mean_energy-median(data$energy)))

data1 <- data %>%
  mutate(energy = ifelse(energy >= mean_energy, 1, 0)) %>%
  select_if(is.numeric)
head(data1)

table(data1$energy)
```

## Classification model - Support Vector Machine (SVM)

### Why SVM?

SVM preforms well in high-dimensional spaces, making it suitable for dataset with many features.
Also it aims to find best decision boundary, that maximizes margin between classes (reduces overfitting).
Performs well by focusing on the most informative features.
It can also handle inbalanced datasets, but our classification solves this issue already.
And our classification is binary, so linear kernel of SVM is suitable for our case. 

### Prepare data for SVM

SVM model does not require normally distributed data. However, it is crucial to properly scale input data. Scale of song features can influence decision boudnaries of SVM, thereby affecting its optimal performance.

Common practice is to scale input to similar range (e.g. 0-1) before training.

Valence values are already normalized, but we scale it also with loudness, so both have $min=0$ and $max=1$.

```{r}
hist(data1$loudness + data1$valence, col = "green") # hypergeometric distribution of data

range(data1$loudness)
range(data1$valence)
```

#### Scaling

```{r}
min_max_scale <- function(x, min_val, max_val) {
  (x - min_val) / (max_val - min_val)
}

data1 <- data1 %>%
  mutate(
    loudness = min_max_scale(loudness, min(loudness), max(loudness)),
    valence = min_max_scale(valence, min(valence), max(valence))
  )

# New range of loudness:
range(data1$loudness)
# New range of valence:
range(data1$valence)
```

### Sampling

```{r}
sample <- sample(c(TRUE, FALSE), nrow(data1), replace = TRUE, prob = c(0.7, 0.3))
train_data <- data1[sample, ]
test_data <- data1[!sample, ]
```

### Train SVM model

Model attributes:

-   **kernel** = "linear" → for binary classification
-   **cost** - higher → more complex decision boundary → penalizes misclassification
-   **scale** = FALSE → inputs are already scaled, so scalling in SVM can be disabled

```{r}
model_svm <- svm(as.factor(energy) ~ loudness + valence, data = train_data, kernel = "linear", cost = 10, scale = FALSE)
```

Support vectors are data points that lies closest to decision boundary and influence placement of decision boundary.

```{r}
summary(model_svm)
```

Coefficients represents weight of each feature to predict true positive/negative. (Intercept) determines the position of decision boundary.

-   positive coef. → higher chance of class 1
-   negative coef. → opposite

Higher coeficient of *loudness* feature supports our **Hypothessis 1**, which has high impact on evaluation that song is more energetic.
But *valence* also take a little role there.

**So we are not rejecting hypothesis, that louder and hapier songs are more energetic.**

```{r}
coefs <- coef(model_svm) ; coefs

plot(model_svm, train_data, loudness ~ valence, col = c("#ffc0c9", "lightgrey"))
# ablines from https://www.datacamp.com/tutorial/support-vector-machines-r
abline(-coefs[1] / coefs[2], (-coefs[3] / coefs[2]), col = "green")
abline((-coefs[1] - 1) / coefs[2], -coefs[3] / coefs[2], lty = 2, col = "green")
abline((-coefs[1] + 1) / coefs[2], -coefs[3] / coefs[2], lty = 2, col = "green")
```

If the decision boundary is nearly horizontal/vertical, it indicates high correlation among the data in the formula, supporting our initial assumption from correlation analysis.

Margin (dashed line) is defined as distance between decision boundary and the closest support vector(s).
SVM aims to maximize this margin during training.
Larger margin indicates better separation between classes and typically leads to better generalization performance.

### Test SVM model

Decision values are signed distance of each data point to the decision boundary. It represents confidence of the model in its prediction.

```{r}
predicted_classes <- predict(model_svm, newdata = test_data, decision.values = TRUE)

decision_values <- attr(predicted_classes, "decision.values")[, 1]
head(decision_values)
```

#### ROC curve

ROC curve is graphical representation of performance of binary classifier. 

```{r message=FALSE}
roc <- roc(test_data$energy, decision_values)
plot(roc, main = "ROC Curve for SVM Model", col = "green", lwd = 2)
legend("bottomright", legend = paste("AUC =", round(auc(roc), 5)), col = "green", lwd = 2, bty = "n")
```

#### Evaluation of SVM

```{r}
confusion_matrix <- caret::confusionMatrix(as.factor(predicted_classes), as_factor(test_data$energy), positive = "1")
ct <- confusion_matrix$table

TP <- ct[2, 2] # true positive
FP <- ct[2, 1] # false positive
TN <- ct[1, 1] # true negative
FN <- ct[1, 2] # false negative
P <- TP + FN # all positives (1)
N <- FP + TN # all negatives (0)

# confusion matrix heatmap
ggplot(as.data.frame(ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")
```

**Accuracy** indicates overall model performance. Higher accuracy suggests better predictive performance, but it may not provide complete picture, especially in imbalanced datasets.

```{r}
print(paste("Accuracy: ", (TP + TN) / (P + N)))
```

But in our case, dataset is balanced, because classification classes are divided nearly 50/50. So our model has good performance.

```{r}
print(paste("Proportion of class 1: ", P / (P + N)))
print(paste("Proportion of class 0: ", N / (P + N)))
```

**Precision** measures ability of model to avoid FP. Higher precision indicates fewer FP, meaning fewer instances are incorrectly predicted as *class 1*.

```{r}
precision <- TP / (FP + TP)
print(paste("Precision: ", precision))
```

**Recall** measures ability of model to capture all actual positives. Higher recall indicates that model captures higher proportion of actual *class 1* instances.

```{r}
recall <- TP / P
print(paste("Recall: ", recall))
```

**F1-score** provides balanced measure between precision and recall. It is useful when there is an uneven class distribution (in our case it is even).

```{r}
f1_score <- 2 * (precision * recall) / (precision + recall)
print(paste("F1 score: ", f1_score))
```

**Specificity** measures how well model performs in correct identified instances of the negative class. High specificity indicates that model has low FP rate, meaning it effectively identifies TN without misclassifying them as positives.

```{r}
print(paste("Specificity: ", TN / (TN + FP)))
```

## Regression models

### randomForest: acousticness \~ loudness + energy + valence

```{r}
randomForest_data <- data[, c("acousticness", "loudness", "energy", "danceability")]
randomForest_data <- randomForest_data %>%
  mutate(
    loudness = min_max_scale(loudness, min(loudness), max(loudness)),
    energy = min_max_scale(energy, min(energy), max(energy)),
    danceability = min_max_scale(danceability, min(danceability), max(danceability))
  )

mean(randomForest_data$acousticness)
median(randomForest_data$acousticness)
range(randomForest_data$acousticness)
range(randomForest_data$danceability)
range(randomForest_data$loudness)
range(randomForest_data$energy)


set.seed(1)
randomForest_train_indices <- createDataPartition(randomForest_data$acousticness, p = 0.8, list = FALSE)
randomForest_train_data <- randomForest_data[randomForest_train_indices, ]
randomForest_test_data <- randomForest_data[-randomForest_train_indices, ]

```

```{r}
randomForest_model <- randomForest(acousticness ~ ., data = randomForest_train_data, ntree=500)
summary(randomForest_model)

randomForest_predictions <- predict(randomForest_model, newdata = randomForest_test_data)
randomForest_mse <- mean((randomForest_test_data$acousticness - randomForest_predictions)^2)
randomForest_mse

varImpPlot(randomForest_model)

randomForest_residuals <- randomForest_test_data$acousticness - randomForest_predictions
plot(randomForest_predictions, randomForest_residuals, xlab = "Predicted", ylab = "Residuals", main = "Residual Plot")

```

### randomForest: energy \~ loudness + valence

```{r}
hist(data$energy, col="green")

model2_data <- data[, c("energy", "loudness", "valence")]
model2_data <- model2_data %>%
  mutate(
    #energy = min_max_scale(energy, min(energy), max(energy)),
    loudness = min_max_scale(loudness, min(loudness), max(loudness)),
    valence = min_max_scale(valence, min(valence), max(valence))
  )

range(model2_data$energy)
range(model2_data$loudness)
range(model2_data$valence)


set.seed(1)
model2_train_indices <- createDataPartition(randomForest_data$acousticness, p = 0.8, list = FALSE)
model2_train_data <- model2_data[model2_train_indices, ]
model2_test_data <- model2_data[-model2_train_indices, ]

```

```{r}
model2 <- randomForest(energy ~ ., data = model2_train_data, ntree=500)
summary(model2)

model2_predictions <- predict(model2, newdata = model2_test_data)
model2_mse <- mean((model2_test_data$energy - model2_predictions)^2)
model2_mse
model2_rmse <- sqrt(model2_mse)
model2_rmse

varImpPlot(model2)

model2_residuals <- model2_test_data$energy - model2_predictions
plot(model2_predictions, model2_residuals, xlab = "Predicted", ylab = "Residuals", main = "Residual Plot")

```

## Back to Classification

### QDA: acousticness \~ loudness + energy + danceability

```{r}
hist(data$acousticness, col="green")
range(data$acousticness)
range(data$danceability)
range(data$loudness)
range(data$energy)

QDA_data <- data[, c("acousticness", "loudness", "energy", "danceability")]

mean_acousticness <- mean(QDA_data$acousticness)
mean_acousticness
median_acousticness <- median(QDA_data$acousticness)
median_acousticness

QDA_data <- QDA_data %>%
  mutate(
    acousticness = ifelse(acousticness >= mean_acousticness, 1, 0),
    loudness = min_max_scale(loudness, min(loudness), max(loudness)),
    energy = min_max_scale(energy, min(energy), max(energy)),
    danceability = min_max_scale(danceability, min(danceability), max(danceability))
  )

range(QDA_data$acousticness)
range(QDA_data$danceability)
range(QDA_data$loudness)
range(QDA_data$energy)

table(QDA_data$acousticness)


set.seed(1)
QDA_train_indices <- createDataPartition(QDA_data$acousticness, p = 0.8, list = FALSE)
QDA_train_data <- QDA_data[QDA_train_indices, ]
QDA_test_data <- QDA_data[-QDA_train_indices, ]

QDA_model <- qda(acousticness ~ ., data = QDA_train_data)
summary(QDA_model)

# Make predictions on the test data
QDA_predictions <- predict(QDA_model, newdata = QDA_test_data, decision.values = TRUE)
QDA_probabilities <- predict(QDA_model, newdata = QDA_test_data, type="probs")
QDA_decision_values <- QDA_probabilities$posterior[,"1"]
head(QDA_decision_values)

QDA_roc <- roc(QDA_test_data$acousticness, QDA_decision_values)
plot(QDA_roc, main = "ROC Curve for QDA Model", col = "green", lwd = 2)
legend("bottomright", legend = paste("AUC =", round(auc(QDA_roc), 5)), col = "green", lwd = 2, bty = "n")

# Calculate accuracy or other classification metrics
QDA_accuracy <- mean(QDA_predictions$class == QDA_test_data$acousticness)

# Print the accuracy
cat("Accuracy:", QDA_accuracy, "\n")

QDA_confusion_matrix <- caret::confusionMatrix(as.factor(QDA_predictions$class),
                                                  as.factor(QDA_test_data$acousticness))
QDA_ct <- QDA_confusion_matrix$table
QDA_TP <- QDA_ct[2, 2] # true positive
QDA_FP <- QDA_ct[2, 1] # false positive
QDA_TN <- QDA_ct[1, 1] # true negative
QDA_FN <- QDA_ct[1, 2] # false negative
QDA_P <- QDA_TP + QDA_FN # all positives (1)
QDA_N <- QDA_FP + QDA_TN # all negatives (0)

ggplot(as.data.frame(QDA_ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")

```

### QDA: median

```{r}
QDA_median_data <- data[, c("acousticness", "loudness", "energy", "danceability")]
median_acousticness <- median(QDA_median_data$acousticness)
median_acousticness


QDA_median_data <- QDA_median_data %>%
  mutate(
    acousticness = ifelse(acousticness >= median_acousticness, 1, 0),
    loudness = min_max_scale(loudness, min(loudness), max(loudness)),
    energy = min_max_scale(energy, min(energy), max(energy)),
    danceability = min_max_scale(danceability, min(danceability), max(danceability))
  )


table(QDA_median_data$acousticness)

range(QDA_median_data$acousticness)
range(QDA_median_data$danceability)
range(QDA_median_data$loudness)
range(QDA_median_data$energy)


set.seed(1)
QDA_median_train_indices <- createDataPartition(QDA_median_data$acousticness, p = 0.8, list = FALSE)
QDA_median_train_data <- QDA_median_data[QDA_median_train_indices, ]
QDA_median_test_data <- QDA_median_data[-QDA_median_train_indices, ]

QDA_median_model <- qda(acousticness ~ ., data = QDA_median_train_data)
summary(QDA_median_model)

# Make predictions on the test data
QDA_median_predictions <- predict(QDA_median_model, newdata = QDA_median_test_data, decision.values = TRUE)
QDA_median_probabilities <- predict(QDA_median_model, newdata = QDA_median_test_data, type="probs")
QDA_median_decision_values <- QDA_median_probabilities$posterior[,"1"]
head(QDA_median_decision_values)

QDA_median_roc <- roc(QDA_median_test_data$acousticness, QDA_median_decision_values)
plot(QDA_median_roc, main = "ROC Curve for QDA Model", col = "green", lwd = 2)
legend("bottomright", legend = paste("AUC =", round(auc(QDA_median_roc), 5)), col = "green", lwd = 2, bty = "n")

# Calculate accuracy or other classification metrics
QDA_median_accuracy <- mean(QDA_median_predictions$class == QDA_median_test_data$acousticness)

# Print the accuracy
cat("Accuracy:", QDA_median_accuracy, "\n")

QDA_median_confusion_matrix <- caret::confusionMatrix(as.factor(QDA_median_predictions$class),
                                                  as.factor(QDA_median_test_data$acousticness))
QDA_median_ct <- QDA_median_confusion_matrix$table
QDA_median_TP <- QDA_median_ct[2, 2] # true positive
QDA_median_FP <- QDA_median_ct[2, 1] # false positive
QDA_median_TN <- QDA_median_ct[1, 1] # true negative
QDA_median_FN <- QDA_median_ct[1, 2] # false negative
QDA_median_P <- QDA_median_TP + QDA_median_FN # all positives (1)
QDA_median_N <- QDA_median_FP + QDA_median_TN # all negatives (0)

ggplot(as.data.frame(QDA_median_ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")
```

### GBM

```{r}
hist(data$acousticness, col="green")
range(data$acousticness)
range(data$danceability)
range(data$loudness)
range(data$energy)

GBM_data <- data[, c("acousticness", "loudness", "energy", "danceability")]

mean_acousticness <- mean(GBM_data$acousticness)
mean_acousticness
median_acousticness <- median(GBM_data$acousticness)
median_acousticness

GBM_data <- GBM_data %>%
  mutate(
    acousticness = ifelse(acousticness >= mean_acousticness, 1, 0),
    loudness = min_max_scale(loudness, min(loudness), max(loudness)),
    energy = min_max_scale(energy, min(energy), max(energy)),
    danceability = min_max_scale(danceability, min(danceability), max(danceability))
  )

range(GBM_data$acousticness)
range(GBM_data$danceability)
range(GBM_data$loudness)
range(GBM_data$energy)


set.seed(1)
GBM_train_indices <- createDataPartition(GBM_data$acousticness, p = 0.8, list = FALSE)
GBM_train_data <- GBM_data[GBM_train_indices, ]
GBM_test_data <- GBM_data[-GBM_train_indices, ]

GBM_model <- gbm(acousticness ~ ., data = GBM_train_data, distribution = "bernoulli", n.trees = 100, interaction.depth = 3)
summary(GBM_model)

# Make predictions on the test data
GBM_prediction_probabilities <- predict(GBM_model, newdata = GBM_test_data, n.trees = 100, type = "response")
head(GBM_prediction_probabilities)

GBM_predictions <- ifelse(GBM_prediction_probabilities >= 0.713, 1, 0)



GBM_roc <- roc(GBM_test_data$acousticness, GBM_prediction_probabilities)
plot(GBM_roc, main = "ROC Curve for QDA Model", col = "green", lwd = 2)
legend("bottomright", legend = paste("AUC =", round(auc(GBM_roc), 5)), col = "green", lwd = 2, bty = "n")

# Calculate accuracy or other classification metrics
GBM_accuracy <- mean(GBM_predictions == GBM_test_data$acousticness)

# Print the accuracy
cat("Accuracy:", GBM_accuracy, "\n")

GBM_confusion_matrix <- caret::confusionMatrix(as.factor(GBM_predictions),
                                               as.factor(GBM_test_data$acousticness))
GBM_ct <- GBM_confusion_matrix$table
GBM_TP <- GBM_ct[2, 2] # true positive
GBM_FP <- GBM_ct[2, 1] # false positive
GBM_TN <- GBM_ct[1, 1] # true negative
GBM_FN <- GBM_ct[1, 2] # false negative
GBM_P <- GBM_TP + GBM_FN # all positives (1)
GBM_N <- GBM_FP + GBM_TN # all negatives (0)

ggplot(as.data.frame(GBM_ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")
```

### Energy Classification QDA vs. GBM

```{r}
hist(data$energy, col="green")
range(data$valence)

QDA_data <- data[, c("energy", "loudness", "valence")]

mean_energy <- mean(QDA_data$energy)
mean_energy
median_energy <- median(QDA_data$energy)
median_energy

QDA_data <- QDA_data %>%
  mutate(
    energy = ifelse(energy >= mean_energy, 1, 0),
    loudness = min_max_scale(loudness, min(loudness), max(loudness)),
    valence = min_max_scale(valence, min(valence), max(valence))
  )

range(QDA_data$energy)
range(QDA_data$valence)
range(QDA_data$loudness)


table(QDA_data$energy)


set.seed(1)
QDA_train_indices <- createDataPartition(QDA_data$energy, p = 0.8, list = FALSE)
QDA_train_data <- QDA_data[QDA_train_indices, ]
QDA_test_data <- QDA_data[-QDA_train_indices, ]

QDA_model <- qda(energy ~ ., data = QDA_train_data)
summary(QDA_model)

# Make predictions on the test data
QDA_predictions <- predict(QDA_model, newdata = QDA_test_data, decision.values = TRUE)
QDA_probabilities <- predict(QDA_model, newdata = QDA_test_data, type="probs")
QDA_decision_values <- QDA_probabilities$posterior[,"1"]
head(QDA_decision_values)

QDA_roc <- roc(QDA_test_data$energy, QDA_decision_values)
plot(QDA_roc, main = "ROC Curve for QDA Model", col = "green", lwd = 2)
legend("bottomright", legend = paste("AUC =", round(auc(QDA_roc), 5)), col = "green", lwd = 2, bty = "n")

# Calculate accuracy or other classification metrics
QDA_accuracy <- mean(QDA_predictions$class == QDA_test_data$energy)


# Print the accuracy
cat("Accuracy:", QDA_accuracy, "\n")

QDA_confusion_matrix <- caret::confusionMatrix(as.factor(QDA_predictions$class),
                                               as.factor(QDA_test_data$energy))
QDA_ct <- QDA_confusion_matrix$table
QDA_TP <- QDA_ct[2, 2] # true positive
QDA_FP <- QDA_ct[2, 1] # false positive
QDA_TN <- QDA_ct[1, 1] # true negative
QDA_FN <- QDA_ct[1, 2] # false negative
QDA_P <- QDA_TP + QDA_FN # all positives (1)
QDA_N <- QDA_FP + QDA_TN # all negatives (0)

ggplot(as.data.frame(QDA_ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")
```

### Trying the median

```{r}
QDA_median_data <- data[, c("energy", "loudness", "valence")]
median_energy <- median(QDA_median_data$energy)
median_energy


QDA_median_data <- QDA_median_data %>%
  mutate(
    energy = ifelse(energy >= median_energy, 1, 0),
    loudness = min_max_scale(loudness, min(loudness), max(loudness)),
    valence = min_max_scale(valence, min(valence), max(valence))
  )



range(QDA_median_data$energy)
range(QDA_median_data$valence)
range(QDA_median_data$loudness)
range(QDA_median_data$energy)


set.seed(1)
QDA_median_train_indices <- createDataPartition(QDA_median_data$energy, p = 0.8, list = FALSE)
QDA_median_train_data <- QDA_median_data[QDA_median_train_indices, ]
QDA_median_test_data <- QDA_median_data[-QDA_median_train_indices, ]

QDA_median_model <- qda(energy ~ ., data = QDA_median_train_data)
summary(QDA_median_model)

# Make predictions on the test data
QDA_median_predictions <- predict(QDA_median_model, newdata = QDA_median_test_data, decision.values = TRUE)
QDA_median_probabilities <- predict(QDA_median_model, newdata = QDA_median_test_data, type="probs")
QDA_median_decision_values <- QDA_median_probabilities$posterior[,"1"]
head(QDA_median_decision_values)

QDA_median_roc <- roc(QDA_median_test_data$energy, QDA_median_decision_values)
plot(QDA_median_roc, main = "ROC Curve for QDA Model", col = "green", lwd = 2)
legend("bottomright", legend = paste("AUC =", round(auc(QDA_median_roc), 5)), col = "green", lwd = 2, bty = "n")

# Calculate accuracy or other classification metrics
QDA_median_accuracy <- mean(QDA_median_predictions$class == QDA_median_test_data$energy)

# Print the accuracy
cat("Accuracy:", QDA_median_accuracy, "\n")

QDA_median_confusion_matrix <- caret::confusionMatrix(as.factor(QDA_median_predictions$class),
                                                      as.factor(QDA_median_test_data$energy))
QDA_median_ct <- QDA_median_confusion_matrix$table
QDA_median_TP <- QDA_median_ct[2, 2] # true positive
QDA_median_FP <- QDA_median_ct[2, 1] # false positive
QDA_median_TN <- QDA_median_ct[1, 1] # true negative
QDA_median_FN <- QDA_median_ct[1, 2] # false negative
QDA_median_P <- QDA_median_TP + QDA_median_FN # all positives (1)
QDA_median_N <- QDA_median_FP + QDA_median_TN # all negatives (0)

ggplot(as.data.frame(QDA_median_ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")
```

### GMB

```{r}
hist(data$energy, col="green")
range(data$energy)
range(data$valence)
range(data$loudness)
range(data$energy)

GBM_data <- data[, c("energy", "loudness", "valence")]

mean_energy <- mean(GBM_data$energy)
mean_energy
median_energy <- median(GBM_data$energy)
median_energy

GBM_data <- GBM_data %>%
  mutate(
    energy = ifelse(energy >= median_energy, 1, 0),
    loudness = min_max_scale(loudness, min(loudness), max(loudness)),
    valence = min_max_scale(valence, min(valence), max(valence))
  )

range(GBM_data$energy)
range(GBM_data$valence)
range(GBM_data$loudness)
range(GBM_data$energy)


set.seed(1)
GBM_train_indices <- createDataPartition(GBM_data$energy, p = 0.8, list = FALSE)
GBM_train_data <- GBM_data[GBM_train_indices, ]
GBM_test_data <- GBM_data[-GBM_train_indices, ]

GBM_model <- gbm(energy ~ ., data = GBM_train_data, distribution = "bernoulli", n.trees = 100, interaction.depth = 3)
summary(GBM_model)

# Make predictions on the test data
GBM_prediction_probabilities <- predict(GBM_model, newdata = GBM_test_data, n.trees = 100, type = "response")
head(GBM_prediction_probabilities)


GBM_roc <- roc(GBM_test_data$energy, GBM_prediction_probabilities)
plot(GBM_roc, main = "ROC Curve for QDA Model", col = "green", lwd = 2)
legend("bottomright", legend = paste("AUC =", round(auc(GBM_roc), 5)), col = "green", lwd = 2, bty = "n")

GBM_predictions <- ifelse(GBM_prediction_probabilities >= 0.8576, 1, 0)

# Calculate accuracy or other classification metrics
GBM_accuracy <- mean(GBM_predictions == GBM_test_data$energy)

# Print the accuracy
cat("Accuracy:", GBM_accuracy, "\n")

GBM_confusion_matrix <- caret::confusionMatrix(as.factor(GBM_predictions),
                                               as.factor(GBM_test_data$energy))
GBM_ct <- GBM_confusion_matrix$table
GBM_TP <- GBM_ct[2, 2] # true positive
GBM_FP <- GBM_ct[2, 1] # false positive
GBM_TN <- GBM_ct[1, 1] # true negative
GBM_FN <- GBM_ct[1, 2] # false negative
GBM_P <- GBM_TP + GBM_FN # all positives (1)
GBM_N <- GBM_FP + GBM_TN # all negatives (0)

ggplot(as.data.frame(GBM_ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")
```

### Tuning Hyper parameters

```{r, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# Define the parameter grid
param_grid <- expand.grid(
  n.trees = c(100, 200, 300),
  interaction.depth = c(3, 4, 5),
  shrinkage = c(0.01, 0.1, 0.3),
  n.minobsinnode = c(10, 20, 30)
)

# Specify the control parameters for training
ctrl <- trainControl(method = "cv", number = 5)

# Perform hyperparameter tuning
garbage <- capture.output(gbm_tuned <- train(
  energy ~ ., 
  data = GBM_train_data, 
  method = "gbm", 
  trControl = ctrl, 
  tuneGrid = param_grid,
))

GBM_tuned_model <- gbm_tuned$finalModel
View(GBM_tuned_model)
```

### Tuned GBM results

We are really not sure about these results, but after optimizing the hyperparameters the GBM seems to have a 100% accuracy on the test set. The prediction probabilites are very nicely separated - either they are very close to 0.1 or very close to 0.9, creating a clear split in the predicted classes.

The one caveat that could mean that something is actually wrong is that in the tuning phase we forgot to specify that the problem is classification and nor regression. It therefore treats the predicted variable as a continuous feature. But hey! it seems to work...

```{r}
GBM_tuned_prediction_probabilities <- predict(GBM_tuned_model, newdata = GBM_test_data, type = "response")
head(GBM_tuned_prediction_probabilities)
GBM_tuned_predictions <- ifelse(GBM_tuned_prediction_probabilities >= 0.8, 1, 0)


GBM_tuned_roc <- roc(GBM_test_data$energy, GBM_tuned_prediction_probabilities)
plot(GBM_tuned_roc, main = "ROC Curve for GBM Model", col = "green", lwd = 2)
legend("bottomright", legend = paste("AUC =", round(auc(GBM_tuned_roc), 5)), col = "green", lwd = 2, bty = "n")

# Calculate accuracy or other classification metrics
GBM_tuned_accuracy <- mean(GBM_tuned_predictions == GBM_test_data$energy)

# Print the accuracy
cat("Accuracy:", GBM_tuned_accuracy, "\n")

GBM_tuned_confusion_matrix <- caret::confusionMatrix(as.factor(GBM_tuned_predictions),
                                               as.factor(GBM_test_data$energy))
GBM_tuned_ct <- GBM_tuned_confusion_matrix$table
GBM_tuned_TP <- GBM_tuned_ct[2, 2] # true positive
GBM_tuned_FP <- GBM_tuned_ct[2, 1] # false positive
GBM_tuned_TN <- GBM_tuned_ct[1, 1] # true negative
GBM_tuned_FN <- GBM_tuned_ct[1, 2] # false negative
GBM_tuned_P <- GBM_tuned_TP + GBM_tuned_FN # all positives (1)
GBM_tuned_N <- GBM_tuned_FP + GBM_tuned_TN # all negatives (0)

ggplot(as.data.frame(GBM_tuned_ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")
```

## Summary

Gradient Boosting Machine seems to be the best performing model when tuned. We are not sure of its results...

The QDA models' performance seems to be too dependent on the way the data is distributed - mainly in the regression problems. This is probably because we don't have any hyperparameters to tune.

Overall the median seems to be the best value to choose when creating a binary classification problem from a continuous numeric variable - it does make sense - the number of entries in the classes is equal.
